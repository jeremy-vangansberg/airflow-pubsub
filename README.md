# Airflow & Google Cloud Platform Demo

Ce dÃ©pÃ´t fournit une solution complÃ¨te permettant de dÃ©marrer rapidement avec Apache Airflow, Google Cloud Platform (GCP), Pub/Sub, Cloud Functions et BigQuery. Le DAG Airflow est dÃ©clenchÃ© automatiquement lorsqu'un message est publiÃ© sur un topic Pub/Sub via une Cloud Function. L'API Airflow est exposÃ©e localement avec `ngrok` pour faciliter les tests.

---

## ğŸ› ï¸ PrÃ©requis

- [Google Cloud CLI](https://cloud.google.com/sdk/docs/install)
- [Terraform](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli)
- [Docker & Docker Compose](https://docs.docker.com/get-docker/)
- [ngrok](https://ngrok.com/download)

---

## ğŸ“¦ Installation

### 1. Configurer votre compte GCP

CrÃ©ez un projet sur [Google Cloud Platform](https://console.cloud.google.com) et activez les APIs suivantes :

- Pub/Sub
- Cloud Functions
- Eventarc
- BigQuery

### 2. Initialisation de l'infrastructure avec Terraform

Ce dÃ©pÃ´t inclut une configuration Terraform pour automatiser la crÃ©ation des ressources GCP nÃ©cessaires :

- Compte de service Airflow avec les rÃ´les Pub/Sub et BigQuery.
- Topic et abonnement Pub/Sub.
- Dataset BigQuery.

ExÃ©cutez ces commandes depuis le dossier `terraform`:

```bash
terraform init
terraform apply
```

### 3. DÃ©marrer Apache Airflow

Utilisez le fichier Docker Compose provenant de la [documentation officielle Airflow](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html) :

```bash
docker-compose up -d
```

AccÃ©dez Ã  l'UI Airflow sur [http://localhost:8080](http://localhost:8080).

### 4. Exposer Airflow localement via ngrok

Exposez l'API Airflow via ngrok pour la rendre accessible depuis Google Cloud :

```bash
ngrok http 8080
```

L'interface web d'ngrok sera disponible sur [http://127.0.0.1:4040](http://127.0.0.1:4040).

### 5. DÃ©ployer la Cloud Function

Un script dynamique rÃ©cupÃ¨re automatiquement l'URL publique ngrok pour dÃ©ployer la Cloud Function :

```bash
./deploy_trigger_airflow.sh
```

CrÃ©ez un fichier `.env` dans le dossier `/scripts/trigger_airflow_dag` avec :

```env
AIRFLOW_USER=votre_user
AIRFLOW_PASS=votre_password
```

---

## ğŸš€ Fonctionnement Global (Mermaid)

```mermaid
graph TD
    Client -->|Publie Message| PubSub[Pub/Sub Topic]
    PubSub -->|DÃ©clenche automatiquement| CloudFunction[Cloud Function Python]
    CloudFunction -->|Appel POST API via ngrok| Airflow[Airflow API]
    Airflow -->|ExÃ©cute| DAG[Airflow DAG]
    DAG -->|Insert data| BigQuery[Dataset BigQuery]
```

---

## ğŸ§ª VÃ©rifications

- Consultez les logs de la Cloud Function sur [Google Cloud Console](https://console.cloud.google.com/functions).
- VÃ©rifiez que le DAG Airflow est dÃ©clenchÃ© correctement depuis l'[UI Airflow](http://localhost:8080).
- VÃ©rifiez les donnÃ©es insÃ©rÃ©es dans votre dataset BigQuery via [Google BigQuery Console](https://console.cloud.google.com/bigquery).

---

## ğŸ“š Ressources complÃ©mentaires

- [Apache Airflow Documentation](https://airflow.apache.org/docs/apache-airflow/stable/)
- [Google Cloud Functions](https://cloud.google.com/functions)
- [Terraform Documentation](https://developer.hashicorp.com/terraform/docs)
- [ngrok Documentation](https://ngrok.com/docs)

---

## ğŸ“‚ Structure du Projet

```
.
â”œâ”€â”€ dags
â”‚   â””â”€â”€ giftcard_ingestion_dag.py
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ deploy_trigger_airflow.sh
â”‚   â””â”€â”€ trigger_airflow_dag
â”‚       â”œâ”€â”€ .env
â”‚       â””â”€â”€ main.py
â””â”€â”€ terraform
    â”œâ”€â”€ main.tf
    â””â”€â”€ variables.tf
```

---

## ğŸ“ Remarques

- Pensez Ã  arrÃªter `ngrok` aprÃ¨s vos tests :

```bash
pkill ngrok
```

- Ne publiez jamais vos secrets (fichiers `.env`) dans le repository Git.

---

**Bon dÃ©ploiement ğŸš€ !**

